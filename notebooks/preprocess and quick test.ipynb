{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some stuff i learned about data:\n",
    "\n",
    "## in player_raw::\n",
    "# element_type indicates position as integer\n",
    "# status is whether the player is available\n",
    "# team indicates team (alphabetically) \n",
    "# team_id = ???\n",
    "# id is the same id as above\n",
    "\n",
    "\n",
    "## in player_df::\n",
    "# opponent_team is id'd alphabetically\n",
    "# each player is id'd by element\n",
    "# gw indicates gameweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df = pd.read_csv('/Users/petter/Documents/FPL/data/player_data/gw_17.csv')\n",
    "team_df = pd.read_csv('/Users/petter/Documents/FPL/data/team_data/E0_17.csv')\n",
    "\n",
    "#This list translates id ('element' in player_df) to name\n",
    "player_idlist = pd.read_csv('/Users/petter/Documents/FPL/FPL_scraper/data/2017-18/player_idlist.csv')\n",
    "\n",
    "player_raw = pd.read_csv('/Users/petter/Documents/FPL/FPL_scraper/data/2017-18/players_raw.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Prepare a dictionary and df to identify the teams by id\n",
    "pl_teams = team_df.HomeTeam.unique()\n",
    "pl_teams.sort()\n",
    "\n",
    "team_dict = {name:id_nr for name,id_nr in zip(pl_teams,list(range(1,len(pl_teams)+1)))}\n",
    "team_dict_df = pd.DataFrame.from_dict(team_dict,'index')\n",
    "team_dict_df.columns = ['team_id']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "player_df.drop_duplicates(inplace = True) #Some duplicates in the data\n",
    "player_df.rename(columns={'element':'player_id'}, inplace=True)\n",
    "player_df.drop('id',axis=1, inplace = True) #drop useless column\n",
    "\n",
    "# Add some extra data to player_df from player_raw\n",
    "\n",
    "\n",
    "merged_df = player_df.merge(player_raw[['element_type','status','team','id']],\n",
    "                            how = 'left', left_on= 'player_id', right_on = 'id')\n",
    "\n",
    "#merged_df = merged_df.drop(['']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do two merges to append home and away team id seperately\n",
    "team_df = team_df.merge(team_dict_df, how = 'left', left_on = 'HomeTeam', right_index = True)\n",
    "team_df.rename(columns={'team_id':'home_id'}, inplace=True)\n",
    "\n",
    "team_df = team_df.merge(team_dict_df, how = 'left', left_on = 'AwayTeam', right_index = True)\n",
    "team_df.rename(columns={'team_id':'away_id'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the list of features we want to use from the team match stats\n",
    "\n",
    "team_features = ['home_id','away_id','FTHG','FTAG','HS','AS','HST','AST','HF','AF','HC','AC','B365H','B365D','B365D']\n",
    "team_df = team_df[team_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join match data to merged_df for home and away seperately, then concat them\n",
    "\n",
    "home_df = merged_df[merged_df.was_home == True].merge(team_df, how = 'left',\n",
    "                                                      left_on = ['team','opponent_team'], right_on = ['home_id','away_id'])\n",
    "\n",
    "away_df = merged_df[merged_df.was_home == False].merge(team_df, how = 'left',\n",
    "                                                      left_on = ['team','opponent_team'], right_on = ['away_id','home_id'])\n",
    "\n",
    "full_df = pd.concat([home_df,away_df],axis = 0)\n",
    "\n",
    "# Sort values and set multi-level index\n",
    "full_df.dropna(inplace = True) # Get some nas from team-join for some reason?\n",
    "full_df.sort_values(by=['player_id','gw'],inplace=True)\n",
    "full_df.set_index(['player_id','gw'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_raw, target_raw, window_size):\n",
    "    '''\n",
    "    Formats the train_raw data into input sequences of length window_size and with the\n",
    "    columns of train_raw as features. \n",
    "    Output dataX as np.array with shape (n_samples, window_size, n_features),\n",
    "    dataY are the corresponding targets\n",
    "    '''\n",
    "    \n",
    "    dataX, dataY = [], []    \n",
    "    \n",
    "    n_samples = train_raw.shape[0]-window_size\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "            dataX.append(train_raw[i:i+window_size,:])\n",
    "            dataY.append(target_raw[i+window_size])\n",
    "    \n",
    "    dataX = np.array(dataX).reshape((n_samples,window_size,train_raw.shape[1]))\n",
    "    dataY = np.array(dataY)\n",
    "    \n",
    "    return dataX, dataY\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we drop from the analysis\n",
    "drop_columns = ['bps', 'ea_index','fixture',\n",
    "                'ict_index','kickoff_time','kickoff_time_formatted',\n",
    "                'loaned_out','loaned_in','opponent_team',\n",
    "                'round', 'team_a_score', 'team_h_score', 'threat',\n",
    "                'transfers_balance', 'transfers_in', 'transfers_out',\n",
    "                'was_home', 'name', 'team', 'id', 'home_id', 'away_id','status']\n",
    "\n",
    "\n",
    "train_df = full_df.drop(drop_columns,axis=1)\n",
    "\n",
    "# Note: \n",
    "# Dropped columns that might be interesting: \n",
    "# transfer_blanace + transfer_in/transfer_out\n",
    "# status (categorical variable, convert to one-hot maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window = 10\n",
    "\n",
    "\n",
    "\n",
    "totX = np.empty((0,window,train_df.shape[1]))\n",
    "totY = []\n",
    "\n",
    "for p_id in train_df.index.get_level_values('player_id').unique():\n",
    "    \n",
    "    player_df = train_df.loc[p_id]\n",
    "    if player_df.shape[0] >= window:\n",
    "        #Only append values if player has enough gameweeks\n",
    "        player_target = (player_df['total_points'].values/\n",
    "                         player_df['value'].values)\n",
    "        \n",
    "        playerX, playerY = create_dataset(player_df.as_matrix(),\n",
    "                                          player_target,\n",
    "                                          window)\n",
    "        \n",
    "        totX = np.concatenate((totX,playerX), axis = 0)\n",
    "        totY = np.concatenate((totY,playerY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(totX, totY, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSTM(prob_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(128,activation='tanh',input_shape = prob_shape))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.01, decay=0.0)\n",
    "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 128)               91136     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 124,417\n",
      "Trainable params: 124,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8662 samples, validate on 2888 samples\n",
      "Epoch 1/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0016 - mean_absolute_error: 0.0344 - val_loss: 0.0010 - val_mean_absolute_error: 0.0353\n",
      "Epoch 2/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0322 - val_loss: 9.8651e-04 - val_mean_absolute_error: 0.0289\n",
      "Epoch 3/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0324 - val_loss: 0.0011 - val_mean_absolute_error: 0.0357\n",
      "Epoch 4/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0324 - val_loss: 0.0011 - val_mean_absolute_error: 0.0268\n",
      "Epoch 5/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0323 - val_loss: 0.0010 - val_mean_absolute_error: 0.0331\n",
      "Epoch 6/100\n",
      "8662/8662 [==============================] - 50s 6ms/step - loss: 0.0011 - mean_absolute_error: 0.0322 - val_loss: 9.8633e-04 - val_mean_absolute_error: 0.0313\n",
      "Epoch 7/100\n",
      "8662/8662 [==============================] - 48s 5ms/step - loss: 0.0011 - mean_absolute_error: 0.0321 - val_loss: 0.0011 - val_mean_absolute_error: 0.0355\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 8/100\n",
      "8662/8662 [==============================] - 49s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0308 - val_loss: 9.8362e-04 - val_mean_absolute_error: 0.0295\n",
      "Epoch 9/100\n",
      "8662/8662 [==============================] - 50s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0308 - val_loss: 9.8454e-04 - val_mean_absolute_error: 0.0292\n",
      "Epoch 10/100\n",
      "8662/8662 [==============================] - 50s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0307 - val_loss: 9.9841e-04 - val_mean_absolute_error: 0.0326\n",
      "Epoch 11/100\n",
      "8662/8662 [==============================] - 50s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0307 - val_loss: 9.8282e-04 - val_mean_absolute_error: 0.0304\n",
      "Epoch 12/100\n",
      "8662/8662 [==============================] - 51s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0307 - val_loss: 9.8739e-04 - val_mean_absolute_error: 0.0314\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 13/100\n",
      "8662/8662 [==============================] - 50s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0307 - val_loss: 9.8261e-04 - val_mean_absolute_error: 0.0302\n",
      "Epoch 14/100\n",
      "8662/8662 [==============================] - 53s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0306 - val_loss: 9.8324e-04 - val_mean_absolute_error: 0.0306\n",
      "Epoch 15/100\n",
      "8662/8662 [==============================] - 48s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0307 - val_loss: 9.8277e-04 - val_mean_absolute_error: 0.0303\n",
      "Epoch 16/100\n",
      "8662/8662 [==============================] - 48s 6ms/step - loss: 0.0010 - mean_absolute_error: 0.0306 - val_loss: 9.8298e-04 - val_mean_absolute_error: 0.0297\n",
      "Epoch 17/100\n",
      "5711/8662 [==================>...........] - ETA: 16s - loss: 0.0010 - mean_absolute_error: 0.0308"
     ]
    }
   ],
   "source": [
    "x_shape = (totX.shape[1], totX.shape[2])\n",
    "\n",
    "model = getLSTM(x_shape)\n",
    "\n",
    "\n",
    "#model = getMiniModel()\n",
    "model.summary()\n",
    "\n",
    "batch_size = 1\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=8, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=batch_size, epochs=100, verbose=1,\n",
    "          callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02315273],\n",
       "       [0.02315273],\n",
       "       [0.02315273],\n",
       "       ...,\n",
       "       [0.02315273],\n",
       "       [0.02315273],\n",
       "       [0.02315273]], dtype=float32)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025483001379376008"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
